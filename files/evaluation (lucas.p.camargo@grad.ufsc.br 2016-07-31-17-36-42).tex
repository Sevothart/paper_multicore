\section{Experimental Evaluation}
\label{sec:eval}

This section describes the experimental evaluation of the previously described 
implementation. Our objectives are threefold: (i) measure the memory 
consumption of the implementation; (ii) evaluate the run-time overhead of the 
implementation; and (iii) verify the impact of the run-time overhead into 
system schedulability. The next subsections show the obtained results for the 
memory consumption, run-time overhead, and schedulability analysis, 
respectively.

\subsection{Memory Footprint}

For measuring the memory consumption of our implementation, we have executed 
EPOS on top of the EPOSMote III platform~\cite{epos}. EPOSMote III features an 
ARM Cortex-M3 32~MHz processor, with 32~Kb of RAM, and 512~Kb of flash. We used 
the \emph{GNU gcc} compiler for ARM at version 4.4.4 to generate the code. For 
measuring the memory footprint, we used the \emph{GNU objdump tool} available 
together with the gcc toolkit.

Table~\ref{tab:memory} shows the obtained memory footprint for each class (see 
Figure~\ref{fig:uml_class_sync}). The table also shows the total lines of code, 
just to correlate the memory footprint with the implementation. Memory usage is 
split into code, data, and static data sections. Data represents the memory 
consumed by an instance of the corresponding semaphore type, not including the 
footprint of the base classes. The total memory consumption describes the memory 
consumed by the implementation of the semaphore subclass and a single 
corresponding object instance. For instance, the total memory consumption of 
the \texttt{Semaphore\_PIP} is 296~bytes, which represents its own 140~bytes 
summed with \texttt{Semaphore} and \texttt{Semaphore\_RT} memory consumptions. 
It is important to highlight that code from the \texttt{Synchronizer\_Common} 
class is not represented in the Table, because it is inlined into the methods 
that use the base class. It means that its code is implemented in the header 
file to improve the run-time overhead by avoiding explicitly method calls. 

\begin{table}[!ht]
\centering
\caption{Memory footprint (in bytes) and lines of code of the implemented 
classes.}
\begin{tabular}{l l l l l|l l}
% \cline{1-7}
Class	& Code &  Data & Static Data & Total Mem. & 
\multicolumn{2}{c}{Lines of Code} \\ 
			&	   &	   &			 &	Consum.	 & Header & Source \\
\hline
Semaphore 			& 132 & 16 & 0  & 148 & 10 & 10 \\ \hline
Sem. RT		 		& 0   & 8  & 0  & 8   & 24 & 0  \\ \hline
Sem. Ceiling	 	& 0   & 4  & 0  & 4   & 9  & 0  \\ \hline
PIP 				& 140   & 0  & 0  & 140   & 8  & 35 \\ \hline
PCP 				& 144  & 0  & 0  & 144  & 8  & 28 \\ \hline
IPCP 				& 144  & 0  & 0  & 144  & 8  & 28 \\ \hline
SRP 				& 368 & 64  & 68 & 504 & 78 & 18 \\ \hline
\end{tabular}
\label{tab:memory}
\end{table}

\subsection{Run-time Overhead}


For measuring the run-time overhead of the implementations, we used the  
EPOSMote’s 32~MHz timer with ±40ppm accuracy. For each protocol, the test 
consisted of a set of 20 tasks that would try to acquire the same resource in a 
cascade, and subsequently release the resource. Then the relevant code sections 
in the OS were instrumented to account for their execution time. Every 
\textit{p} and \textit{v} method of every semaphore type was instrumented, as 
well as the system code responsible for queuing and dequeuing tasks, which is 
used by the base semaphore implementation. These sections were timed 
with a small helper utility, that wraps the code, setups a hardware timer 
peripheral and uses it to count the amount of time consumed in system clock 
cycles by the code section. We repeated the test 10 times and extract the 
worst-case run-time overhead from these executions.

Figure~\ref{fig:run_time_overhead_eposmote} presents the obtained 
worst-case run-time overhead for each protocol in~\si{\micro\second} to perform 
\textit{p} and \textit{v} operations, as a function of the number of tasks 
waiting on the semaphore. It was observed that for \textit{p} operations of the 
PIP, IPCP, and PCP protocols, the overhead depends significantly on the number 
of tasks currently waiting on the semaphore. This is a consequence of queuing 
tasks when they block waiting on the resource. This queue is implemented as a 
linked list, and as the queue grows larger, the enqueuing overhead increases. 
This queue overhead overcomes the overhead of the protocols' code. However, we 
can note a very small different when there is no tasks waiting on the semaphore. 
In that case, IPCP has a smaller overhead (almost imperceptible in the graph).

\fig{run_time_overhead_eposmote}{The obtained run-time overhead running the 
implemented protocols on the EPOSMote III platform.}{width=\columnwidth}

In contrast, the overhead for the \textit{v} operation does not change with the 
number of tasks in the queue, because dequeuing is always done from the head of 
the queue, so this is a constant-time operation. PCP and IPCP have almost the 
same overhead for the \emph{v} operation, because both protocols do similar 
operations. The overhead for PIP, however, is a little bit larger (up to 
1.2~\si{\micro\second}) due to its more sophisticated operations. The \emph{v} 
operation of the SRP is the worst, because it demands an updating of the system 
ceiling, which is performed in a loop (its size is equal to the number of 
tasks that use the resource, 20 in our experiments). 

The case of tasks being blocked on a semaphore under SRP should never happen in 
principle. This is because of the preemption test, that disables a task from 
beginning execution if one of its resource accesses would cause it to block. 
However, in our practical implementation, non-real-time tasks do not have an 
assigned preemption level, and are unaffected by SRP. Therefore, in the case of 
a soft real-time application that makes mixed use of tasks types, tasks may 
still block on a resource.

\subsection{Schedulability Impact}

For measuring the impact of the run-time overhead into the schedulability of 
task sets, we used a methodology similar to the one proposed 
by Yang et al~\cite{Yang:2015}, adapting their 
methodology to our scenario (\emph{i.e,} uniprocessor system). We generated 
several task sets according to the following rules. A task's period $p_i$ was 
randomly chosen from a log-uniform distribution ranging over 
[10\si{\milli\second}, 100\si{\milli\second}] (\emph{homogeneous periods}) or 
[1\si{\milli\second}, 1000\si{\milli\second}] (\emph{heterogeneous periods}). 
Each task's utilization $u_i$ was randomly chosen from a uniform distribution 
ranging over [0.05\si{\milli\second}, 0.1\si{\milli\second}] (\emph{light 
utilizations }) or [0.1\si{\milli\second}, 0.25\si{\milli\second}] 
(\emph{medium 
utilizations}), and the task's WCET $e_i$ was set to $e_i = p_i \times u_i$. 
The number of shared resources was varied across $n_r \in \{4, 8, 16\}$. For 
each resource, there is a probability $p^{acc}$ associated with each task to 
access the respective resource. We vary the probability across $p^{acc}$ $\in$ 
\{0.1, 0.25, 0.5\}. The maximum critical section length $L_{i,q}$ was chosen 
uniformly from [1\si{\micro\second}, 25\si{\micro\second}] (\emph{short}), 
[25\si{\micro\second}, 100\si{\micro\second}] (\emph{medium}), and 
[100\si{\micro\second}, 500\si{\micro\second}] (\emph{long}).  Each task's job 
uses a resource only once per activation ($N_{i,q}$ = 1). 

For each configuration (108 combinations\footnote{Due to space constraint, 
we only show the graphs of 4 combinations in the paper.}), we generated 1000 
task set per utilization cap, varying it from 0.1 to 1. For each task set and 
utilization cap, we first applied the traditional PIP, PCP, and SRP 
schedulability analyses~\cite{Buttazzo:2011} without considering any run-time 
overhead. IPCP has the same schedulability test as PCP~\cite{Buttazzo:2011}. For 
PIP, PCP, and IPCP we consider the RM scheduling, while for SRP we consider the 
EDF scheduling, to evaluate a dynamic-priority s. Then, we inflated each task's WCET $e_i$ with the obtained 
run-time overhead (see Figure~\ref{fig:run_time_overhead_eposmote}). To obtain 
the correct run-time overhead, we used the following approach. Considering a 
task 
$\tau_i$ and a task set with $n$ tasks. For each resource $R_j$ used by 
$\tau_i$, we iterated over the tasks finding the other tasks that also use 
$R_j$. Then, we obtained the maximum number of tasks that share the same 
resources with $\tau_i$. This gave us the worst-case number of tasks that can 
wait on the same resource. We used this number to take the overhead. We 
implemented the task generation methodology as well as the protocol 
schedulability analyses~\cite{Buttazzo:2011} in the SchedCAT 
tool~\cite{schedcat}\footnote{The scripts used during the experiments are 
available online at \url{http://epos.lisha.ufsc.br.}}. 

Due to the large combination of configurations, we only focus here on the major 
findings. Figures~\ref{fig:label-a}--\ref{fig:label-d} shows the representative 
graphs. On the \emph{x-axis} we vary the utilization cap of the generated task 
sets. On the \emph{y-axis} we vary the schedulability ratio. For instance, a 
schedulability ratio of 0.6 means that 60\% of the 1000 task sets were 
schedulable. Below, we discuss the main observed behaviors from the graphs.

\begin{figure*}
\centering

\begin{minipage}[b]{.48\textwidth}
\includegraphics[width=\textwidth]{fig/{light_heterogeneous_long_4_0.25}.pdf}
\caption{Heterogeneous periods, light utilizations, long critical 
sections, $n_r = 4$, $p^{acc} = 0.25$, and $N = 1$.}\label{fig:label-a}
\end{minipage}\qquad

\begin{minipage}[b]{.48\textwidth}
\includegraphics[width=\textwidth]{fig/{light_heterogeneous_medium_4_0.25}.pdf}
\caption{Heterogeneous periods, light utilizations, medium critical 
sections, $n_r = 4$, $p^{acc} = 0.25$, and , $N = 1$.}\label{fig:label-b}
\end{minipage}\qquad

\begin{minipage}[b]{.48\textwidth}
\includegraphics[width=\textwidth]{fig/{light_heterogeneous_short_4_0.25}.pdf}
\caption{Heterogeneous periods, light utilizations, short critical 
sections, $n_r = 4$, $p^{acc} = 0.25$, and , $N = 1$.}\label{fig:label-c}
\end{minipage}\qquad

\begin{minipage}[b]{.48\textwidth}
\includegraphics[width=\textwidth]{fig/{light_heterogeneous_long_8_0.5}.pdf}
\caption{Heterogeneous periods, light utilizations, long critical 
sections, $n_r = 8$, $p^{acc} = 0.5$, and , $N = 1$.}\label{fig:label-d}
\end{minipage}

\end{figure*}
\begin{figure*}
\centering

\begin{minipage}[b]{.48\textwidth}
\includegraphics[width=\textwidth]{fig/{light_homogeneous_long_8_0.5}.pdf}
\caption{Homogeneous periods, light utilizations, long critical 
sections, $n_r = 8$, $p^{acc} = 0.5$, and , $N = 1$.}\label{fig:label-e}
\end{minipage}\qquad

\begin{minipage}[b]{.48\textwidth}
\includegraphics[width=\textwidth]{fig/{medium_homogeneous_long_8_0.5}.pdf}
\caption{Homogeneous periods, medium utilizations, long critical 
sections, $n_r = 8$, $p^{acc} = 0.5$, and $N = 1$.}\label{fig:label-f}
\end{minipage}\qquad

\end{figure*}

\textbf{Efficient implementation provides schedulability bounds close to the 
theoretical ones.} Our results indicate that proper implementation of resource 
access protocols provide small impact on the schedulability ratio of task sets. 
For instance, in a scenario of high demand for shared resources 
(Figure~\ref{fig:label-a}), the difference of the schedulability ratio with and 
without overhead is less than 10\% (for utilization cap of 0.5). In a scenario 
of low demand for shared resources (Figure~\ref{fig:label-c}), the overhead has 
no impact on the schedulability.

\textbf{Critical section sizes heavily impact schedulability for light-utilization tasksets.} As expected, figures~\ref{fig:label-a}, \ref{fig:label-b}, and~\ref{fig:label-c} show that for task sets with a big number of low-utilization tasks, critical section sizes have a strong impact on the schedulability ratio, with longer critical sections reducing schedulability. This is a consequence of blocking time bounds for all protocols being derived directly from critical section lengths.

\textbf{Homogeneity of task periods has a major effect on the schedulability ratio.}
For a given total utilization, heterogeneous task sets contain tasks with shorter periods. Since critical section lenghts for a given resource are the same for all tasks, short-period tasks were much more affected by blocking-time bound terms on the schedulability analysis. Because of this, the schedulability ratios for task sets with heterogeneous periods were lower.
(see Figures~\ref{fig:label-d} and~\ref{fig:label-e}).

\textbf{Schedulability is higher in medium utilization.} This is mainly 
due the fact that task sets with medium utilizations have fewer tasks when 
compared to light utilizations, so there were less opportunities for resource accesses and blocking (compare Figures~\ref{fig:label-e} and~\ref{fig:label-f}). It can also be observed in Figure~\ref{fig:label-e} that for a higher number of tasks, if periods are homogeneous, PCP performs much better than PIP.

\textbf{Probability of accessing a resource impacts the schedulability of task 
sets considering overhead.} When we used higher probabilities to access shared 
resources, the schedulability of task sets considering the run-time overhead 
decreased. It was possible to perceive a small reduction on the 
schedulability in all protocols in the utilizations caps 0.5 and 0.6.